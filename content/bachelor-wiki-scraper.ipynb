{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contestant Data Scraper\n",
    "\n",
    "Everyone should watch the Bachelor(ette). I blindly live by this axiom and assume that everyone understands the beauty that is *Sister Wives* meets the *Dating Game* meets the Jerry Springer hosted *Baggage* (if you haven't seen this, take a weekend and get back to me). And yes, I will shut down all of you sexists out there who believe this show is better suited for the female audience. That's like saying that sports are only for men, and if you believe this then you are most likely a douchebag and your opinion is nullified by default.\n",
    "\n",
    "But let's get down to the point. Countless number (exageration) of people who watch this show attempt to predict who is going home, when they will go home, and, of course, who is going to win the season. Well, this is the first step in an attempt to build a probabistic model that will forcast the odds of each contestant by learning from past season patterns.  However, there is little structured data about the television show, so I had to collect it myself. Well, I had to set up a webscraper to do it for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I first need to collect all available contestant data at the moment. This can be updated for the current season every single week as updates are made. The easiest way to do this was to go to Wikipedia. Let's first collect the urls of those seasons that have data available..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bachelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bachelor_seasons = np.arange(9,21)\n",
    "\n",
    "bachelor_urls = []\n",
    "for season in bachelor_seasons:\n",
    "    season_url = 'https://en.wikipedia.org/wiki/The_Bachelor_(season_{})'.format(season)\n",
    "    bachelor_urls.append(season_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bachelorette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bachelorette_seasons = np.arange(4,13)\n",
    "bachelorette_seasons = np.insert(bachelorette_seasons,0,2)\n",
    "\n",
    "bachelorette_urls = []\n",
    "for season in bachelorette_seasons:\n",
    "    season_url = 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_{})'.format(season)\n",
    "    bachelorette_urls.append(season_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next two functions aren't really going to make sense at the moment, but essentially I need them to clean up some of the data.  The `getDigits()` function simply collects any numerical digits in a string. The `hasNumbers` function just checks to see if the string has digits in it. This will come in handy if you look closely `dataCollector` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDigits(str1):\n",
    "    c = \"\"\n",
    "    for i in str1:\n",
    "        if i.isdigit():\n",
    "            c += i\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are going to be a string of functions that will be involved in grabbing the data and outputting it to a dataframe and what not. Those are all below.  This first one simply collects the page from a given url and returns the html in lmxl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeSoup(url):\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content,\"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where all of the magic happens! Okay, not really. This is the very important and very boring cleaning of the data as you read it in and then outputting it to a dictionary so that it can easily be formatted to other, more flexible structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataCollector(soup):\n",
    "    import re\n",
    "    # make the soup\n",
    "    this = makeSoup(soup)\n",
    "    \n",
    "    # find the right table\n",
    "    tables = this.findChildren('table')\n",
    "    table = tables[1]\n",
    "    \n",
    "    # turn the data into a workable format\n",
    "    data   = [[td.text for td in row.select('td')]\n",
    "             for row in table.findAll('tr')]\n",
    "    \n",
    "    # create the header row and the body\n",
    "    header = ['name','age', 'hometown', 'occupation', 'elimination']\n",
    "    body = data[1:]\n",
    "    cols   =  zip(*body)\n",
    "    \n",
    "    # create a dict with the data\n",
    "    tbl_d  = {name:col for name, col in zip(header,cols)}\n",
    "    \n",
    "    # extract the season number from the original soup\n",
    "    number = re.findall(r'\\d+', soup)\n",
    "    \n",
    "    # remove brackets\n",
    "    num= ''.join(number)\n",
    "    \n",
    "    # create a new key of seasons\n",
    "    tbl_d['season'] = [num] * len(tbl_d['age'])\n",
    "    \n",
    "    new_names = []\n",
    "    for name in tbl_d['name']:\n",
    "        cleaned_name = re.sub(r'\\[\\w+\\]', ' ', name)\n",
    "        new_names.append(cleaned_name)\n",
    "\n",
    "    tbl_d['name'] = new_names\n",
    "    \n",
    "    # find the first name with last name abbreviation\n",
    "    name_abbreviation = []\n",
    "    \n",
    "    for name in tbl_d['name']:\n",
    "        names = name.split(\" \")\n",
    "\n",
    "        new_names = []\n",
    "        for name in names:\n",
    "            cleaned_name = re.sub(r'\\(\\w+\\)', ' ', name)\n",
    "            new_names.append(cleaned_name)\n",
    "        filtered = filter(lambda items: items.strip(), new_names) # remove blank space items \n",
    "        new_names = list(filtered)\n",
    "\n",
    "        if len(new_names)== 1:\n",
    "            new_name = new_names[0]\n",
    "        else:\n",
    "            new_name = \"{} {}.\".format(new_names[0], new_names[-1][0])\n",
    "        name_abbreviation.append(new_name)\n",
    "        \n",
    "    tbl_d['name_abbreviation'] = name_abbreviation\n",
    "    \n",
    "    # clean up the hometowns\n",
    "    new_hometowns = []\n",
    "    \n",
    "    for hometown in tbl_d['hometown']:\n",
    "        cleaned_hometown = re.sub(r'\\[\\w+\\]', ' ', hometown)\n",
    "        new_hometowns.append(cleaned_hometown)\n",
    "\n",
    "    tbl_d['hometown'] = new_hometowns\n",
    "    \n",
    "    # find just the number of the elimination week\n",
    "    elimination_week = []\n",
    "\n",
    "    for item in tbl_d['elimination']:\n",
    "        if hasNumbers(item):\n",
    "            digits = getDigits(item)\n",
    "            episode = digits[0]\n",
    "        else:\n",
    "            episode = item\n",
    "        elimination_week.append(episode)\n",
    "    tbl_d['elimination'] = elimination_week\n",
    "    \n",
    "    # return dictionary\n",
    "    return tbl_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final function is meant to tie it all together and output it to a dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def frameMaker(urls):\n",
    "    frames = []\n",
    "\n",
    "    for url in urls:\n",
    "        dictionary = dataCollector(url)\n",
    "        frame = pd.DataFrame(dictionary)\n",
    "        frames.append(frame)\n",
    "\n",
    "    combined = pd.concat(frames)\n",
    "    \n",
    "    return combined.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bachelorette_frame = frameMaker(bachelorette_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bachelor_frame = frameMaker(bachelor_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bachelorette_frame.to_csv('data/bachelorette_contestants.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bachelor_frame.to_csv('data/bachelor_contestants.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
